{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sentence Generation from Language Model\n",
    "\n",
    "This tutorial demonstrates how to generate text using a pre-trained language model in the following two ways:\n",
    "\n",
    "- with beam search sampler\n",
    "- with sequence sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Variables to configure when generating sequences:\n",
    "\n",
    "- V = vocabulary size\n",
    "- T = sequence length\n",
    "- the number of possible outcomes to consider a sequence = V^T."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Given a language model, we can generate sequences according to the probability that they would occur according to our model. At each time step, a language model predicts the likelihood of each word occuring, given the context from prior time steps. The outputs at any time step can be any word from the vocabulary whose size is V and thus the number of all possible outcomes for a sequence of length T is thus V^T. \n",
    "\n",
    "While sometimes we might want to generate sentences according to their probability of occuring, at other times we want to find the sentences that *are most likely to occur*. This is especially true in the case of language translation where we don't just want to see *a* translation. We want the *best* translation. While finding the optimal outcome quickly becomes intractable as time step increases, there are still many ways to sample reasonably good sequences. GluonNLP provides two samplers for generating from a language model: SequenceSampler and BeamSearchSampler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load Pretrained Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import gluonnlp as nlp\n",
    "ctx = mx.cpu()\n",
    "lm_model, vocab = nlp.model.get_model(name='awd_lstm_lm_1150',\n",
    "                                      dataset_name='wikitext-2',\n",
    "                                      pretrained=True,\n",
    "                                      ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Sampling a Sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Beam Search Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To overcome the exponential complexity in sequence decoding, beam search decodes greedily, keeping those sequences that are most likely based on the probability up to the current time step. The size of this subset is called the *beam size*. Suppose the beam size is K and the output vocabulary size is V. When selecting the beams to keep, the beam search algorithm first predict all possible successor words from the previous K beams, each of which has V possible outputs. This becomes a total of K\\*V paths. Out of these K\\*V paths, beam search ranks them by their score keeping only the top K paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's get started executing a beam search with GluonNLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BeamSearchSampler in module gluonnlp.model.sequence_sampler:\n",
      "\n",
      "class BeamSearchSampler(builtins.object)\n",
      " |  Draw samples from the decoder by beam search.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  beam_size : int\n",
      " |      The beam size.\n",
      " |  decoder : callable\n",
      " |      Function of the one-step-ahead decoder, should have the form::\n",
      " |  \n",
      " |          outputs, new_states = decoder(step_input, states)\n",
      " |  \n",
      " |      The outputs, input should follow these rules:\n",
      " |  \n",
      " |      - step_input has shape (batch_size,),\n",
      " |      - outputs has shape (batch_size, V),\n",
      " |      - states and new_states have the same structure and the leading\n",
      " |        dimension of the inner NDArrays is the batch dimension.\n",
      " |  eos_id : int\n",
      " |      Id of the EOS token. No other elements will be appended to the sample if it reaches eos_id.\n",
      " |  scorer : BeamSearchScorer, default BeamSearchScorer(alpha=1.0, K=5)\n",
      " |      The score function used in beam search.\n",
      " |  max_length : int, default 100\n",
      " |      The maximum search length.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, inputs, states)\n",
      " |      Sample by beam search.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      inputs : NDArray\n",
      " |          The initial input of the decoder. Shape is (batch_size,).\n",
      " |      states : Object that contains NDArrays\n",
      " |          The initial states of the decoder.\n",
      " |      Returns\n",
      " |      -------\n",
      " |      samples : NDArray\n",
      " |          Samples draw by beam search. Shape (batch_size, beam_size, length). dtype is int32.\n",
      " |      scores : NDArray\n",
      " |          Scores of the samples. Shape (batch_size, beam_size). We make sure that scores[i, :] are\n",
      " |          in descending order.\n",
      " |      valid_length : NDArray\n",
      " |          The valid length of the samples. Shape (batch_size, beam_size). dtype will be int32.\n",
      " |  \n",
      " |  __init__(self, beam_size, decoder, eos_id, scorer=BeamSearchScorer(\n",
      " |  \n",
      " |  ), max_length=100)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nlp.model.BeamSearchSampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Scorer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "scorer = nlp.model.BeamSearchScorer(alpha=0, K=5, from_logits=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The BeamSearchScorer is a simple HybridBlock that implements the scoring function with length penalty in Google NMT paper. \n",
    "```\n",
    "scores = (log_probs + scores) / length_penalty\n",
    "length_penalty = (K + length)^alpha / (K + 1)^alpha\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Decoder Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class LMDecoder(object):\n",
    "    def __init__(self, model):\n",
    "        self._model = model\n",
    "    def __call__(self, inputs, states):\n",
    "        outputs, states = self._model(mx.nd.expand_dims(inputs, axis=0), states)\n",
    "        return outputs[0], states\n",
    "    def state_info(self, *arg, **kwargs):\n",
    "        return self._model.state_info(*arg, **kwargs)\n",
    "decoder = LMDecoder(lm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Beam Search Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Given a scorer and decoder, we are ready to create a sampler. We use symbol '.' to indicate the end of sentence (EOS). We can use vocab to get the index of the EOS, and then feed the index to the sampler. The following codes shows how to construct a beam search sampler. We will create a sampler with 4 beams and a maximum sample length of 20.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "eos_id = vocab['.']\n",
    "beam_sampler = nlp.model.BeamSearchSampler(beam_size=5,\n",
    "                                           decoder=decoder,\n",
    "                                           eos_id=eos_id,\n",
    "                                           scorer=scorer,\n",
    "                                           max_length=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Generate Sequences w/ Beam Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Next, we are going to generate sentences starting with \"I love it\" using beam search first. We feed ['I', 'Love'] to the language model to get the initial states and set the initial input to be the word 'it'. We will then print the top-3 generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "bos = 'I love it'.split()\n",
    "bos_ids = [vocab[ele] for ele in bos]\n",
    "begin_states = lm_model.begin_state(batch_size=1, ctx=ctx)\n",
    "if len(bos_ids) > 1:\n",
    "    _, begin_states = lm_model(mx.nd.expand_dims(mx.nd.array(bos_ids[:-1]), axis=1),\n",
    "                               begin_states)\n",
    "inputs = mx.nd.full(shape=(1,), ctx=ctx, val=bos_ids[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def generate_sequences(sampler, inputs, begin_states, num_print_outcomes):\n",
    "    samples, scores, valid_lengths = sampler(inputs, begin_states)\n",
    "    samples = samples[0].asnumpy()\n",
    "    scores = scores[0].asnumpy()\n",
    "    valid_lengths = valid_lengths[0].asnumpy()\n",
    "    print('Generation Result:')\n",
    "    for i in range(num_print_outcomes):\n",
    "        sentence = bos[:-1]\n",
    "        for ele in samples[i][:valid_lengths[i]]:\n",
    "            sentence.append(vocab.idx_to_token[ele])\n",
    "        print([' '.join(sentence), scores[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation Result:\n",
      "['I love it .', -1.1241299]\n",
      "['I love it \" .', -4.001592]\n",
      "['I love it , but it is not a <unk> .', -15.624882]\n",
      "['I love it , but it is not a <unk> , but it is not a <unk> .', -28.37084]\n",
      "['I love it , but it is not a <unk> , and it is not a <unk> .', -28.826914]\n"
     ]
    }
   ],
   "source": [
    "generate_sequences(beam_sampler, inputs, begin_states, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sequence Sampler\n",
    "\n",
    "The previous generation results may look a bit borning. Now, let's use sequence sampler to get some more interesting results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A SequenceSampler samples from the contextual multinomial distribution produced by the language model at each time step. Since we may want to control how \"sharp\" the distribution is to tradeoff diversity with correctness, we can use the temperature option in SequenceSampler, which controls the temperature of the softmax function.\n",
    "\n",
    "For each input same, sequence sampler can sample multiple independent sequences at once. The number of independent sequences to sample can be specified through the argument `beam_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "seq_sampler = nlp.model.SequenceSampler(beam_size=5,\n",
    "                                        decoder=decoder,\n",
    "                                        eos_id=eos_id,\n",
    "                                        max_length=100,\n",
    "                                        temperature=0.97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Generate Sequences w/ Sequence Sampler\n",
    "Now, use the sequence sampler created to sample sequences based on the same inputs used previously.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation Result:\n",
      "['I love it .', -1.1241299]\n",
      "['I love it and there is nothing enough for you to get public sites on the dam , shore and <unk> , and <unk> the placenames of which he says when going â€“ have been thoroughly dispersed .', -153.92299]\n",
      "['I love it as similar to the B @-@ <unk> , only not to be invited .', -51.27569]\n",
      "['I love it : \" I don \\'t do this because you are making all things <unk> .', -44.835636]\n",
      "['I love it and world mates .', -22.420462]\n"
     ]
    }
   ],
   "source": [
    "generate_sequences(seq_sampler, inputs, begin_states, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Practice\n",
    "\n",
    "- Tweak alpha and K in BeamSearchScorer, how are the results changed?\n",
    "- Try different samples to decode."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
